# Faculty Navigation App

This iOS application helps visually impaired individuals navigate indoor environments using real-time object detection. Built with Swift and powered by YOLO11, the app provides spoken guidance and camera-based detection to help users independently find classrooms and navigate corridors inside the Faculty of Engineering and Natural Sciences (FENS) at SabancÄ± University.

---

## ðŸŽ¯ Project Objective

To assist visually impaired individuals with real-time, camera-based indoor navigation by combining object detection and audio feedback in a fully mobile application.

---

## ðŸ§  Key Technologies

- **YOLO11 (Ultralytics)** â€“ For real-time object detection and corridor identification  
- **Swift (iOS)** â€“ For mobile app development  
- **Roboflow** â€“ For dataset labeling and preprocessing  
- **Google Colab** â€“ For model training  
- **CoreML (.mlpackage)** â€“ For deploying the trained model on iOS  
- **Text-to-Speech (Swift API)** â€“ For voice-based user feedback  

---

## ðŸ› ï¸ Features

- ðŸ” Real-time object detection with YOLO11  
- ðŸ“ Indoor corridor identification using custom-labeled markers  
- ðŸ”Š Voice feedback with tap-based interaction (single, double, triple taps)  
- ðŸ“¸ On-device detection using the phoneâ€™s camera  
- ðŸ§­ Guidance toward classrooms within G Floor at FENS  

---

## ðŸ—ï¸ Dataset & Model

- **Images Collected**: 667  
- **Environment**: G Floor â€“ FENS, SabancÄ± University  
- **Labeled Objects**: vending machine, trash bin, printer, painting, doors, fire safety equipment  
- **Model Training**: Conducted in Google Colab, converted to `.mlpackage` for iOS  
- **Corridor Tracking**: Objects were class-labeled to act as spatial markers for identifying current corridor location  

---

## ðŸ“± Interaction

- **Single Tap** â€“ Get current location  
- **Double Tap** â€“ Request direction to a room  
- **Triple Tap** â€“ Cancel navigation or request help  
- The app provides audio feedback to guide users through corridor transitions.

---

## ðŸ‘¨â€ðŸ’» Contributors

- **Abdallah Al Homsi** â€“ AI integration, detection pipeline, user interface  
- **Imran Hasanzade** â€“ App structure, testing, and data processing  
- **Sadettin Er** â€“ Dataset collection, training setup, user feedback logic  

---

## ðŸ“ˆ Limitations & Future Work

- Currently limited to G Floor due to dataset constraints  
- Future plans include expanding the dataset to additional floors and buildings  
- Planned enhancements: Indoor map integration, multilingual voice support, and wider model deployment

---

## ðŸ“„ Supervisor

This project was completed under the supervision of **Professor Kamer Kaya** as part of the PROJ201 course at SabancÄ± University.

---

## ðŸ“« Contact

For questions or collaboration:
- ðŸ“§ homsiabdullah4@gmail.com  
- ðŸ”— [LinkedIn](https://www.linkedin.com/in/abdallah-al-homsi-817a7834b)  
- ðŸ§‘â€ðŸ’» [GitHub](https://github.com/abdallahalhomsi)

---

> Empowering independence through AI and accessibility.
